{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-07T22:39:13.339967Z",
     "start_time": "2025-05-07T22:39:12.848888Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "from langgraph.graph import MessagesState, StateGraph\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import END\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langchain_core.documents import Document\n",
    "from langchain.chat_models import init_chat_model\n",
    "import time\n",
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "from datetime import datetime\n",
    "from testing.response_eval_tools import set_api_key_from_path, initialize_evaluation_llm, evaluate_model_outputs_from_paths\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import List, Dict, Any, Optional, Union\n",
    "import uuid\n",
    "\n",
    "from qa_pairs.qa_data_loader import load_static_demo_pairs\n",
    "set_api_key_from_path(Path('./testing/gemini-key'))\n",
    "\n",
    "################# [ Validate GPU Available ] #################\n",
    "import torch\n",
    "\n",
    "# assert(torch.cuda.is_available())\n",
    "\n",
    "##############################################################\n",
    "\n",
    "chroma_path = \"./test_db\"\n",
    "collection_name = 'multi_proceedings_test'\n",
    "embedding_model_name = 'all-MiniLM-L6-v2'\n",
    "\n",
    "client = chromadb.PersistentClient(path=chroma_path)\n",
    "embedding_function = SentenceTransformerEmbeddingFunction(model_name=embedding_model_name)\n",
    "collection = client.get_or_create_collection(name=collection_name, embedding_function=embedding_function)\n",
    "\n",
    "llm = initialize_evaluation_llm()\n",
    "# llm = init_chat_model(\"llama3.2:3b-instruct-q8_0\", model_provider=\"ollama\")\n",
    "\n",
    "class ChatHistoryManager:\n",
    "    def __init__(self, max_history_length=10):\n",
    "        self.sessions = {}\n",
    "        self.max_history_length = max_history_length\n",
    "\n",
    "    def get_or_create_session(self, session_id: str) -> List:\n",
    "        \"\"\"Get or create a new chat session\"\"\"\n",
    "        if session_id not in self.sessions:\n",
    "            self.sessions[session_id] = []\n",
    "        return self.sessions[session_id]\n",
    "\n",
    "    def add_message(self, session_id: str, message) -> None:\n",
    "        \"\"\"Add a message to the chat history\"\"\"\n",
    "        history = self.get_or_create_session(session_id)\n",
    "        history.append(message)\n",
    "\n",
    "        if len(history) > self.max_history_length * 2:  # Keep pairs of messages\n",
    "            self.sessions[session_id] = history[-self.max_history_length*2:]\n",
    "\n",
    "    def get_history(self, session_id: str) -> List:\n",
    "        \"\"\"Get the chat history for a session\"\"\"\n",
    "        return self.get_or_create_session(session_id)\n",
    "\n",
    "    def clear_history(self, session_id: str) -> None:\n",
    "        \"\"\"Clear the chat history for a session\"\"\"\n",
    "        self.sessions[session_id] = []\n",
    "\n",
    "    def save_history(self): # TODO: STORE HISTORY TO FILE\n",
    "        pass\n",
    "\n",
    "    def load_history(self): # TODO: LOAD HISTORY FROM FILE\n",
    "        pass\n",
    "\n",
    "chat_manager = ChatHistoryManager(max_history_length=15)\n",
    "\n",
    "# Set up retrieval tool\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve(query: str, k: int = 8):\n",
    "    \"\"\"Retrieve information related to a query.\"\"\"\n",
    "    # Query ChromaDB directly\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=k,\n",
    "    )\n",
    "\n",
    "    # Format results for LangChain compatibility\n",
    "    retrieved_docs = []\n",
    "    for i in range(len(results['ids'][0])):\n",
    "        doc_id = results['ids'][0][i]\n",
    "        content = results['documents'][0][i]\n",
    "        metadata = results['metadatas'][0][i] if results['metadatas'][0] else {}\n",
    "\n",
    "        doc = Document(page_content=content, metadata=metadata)\n",
    "        retrieved_docs.append(doc)\n",
    "\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\n\" f\"Content: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs\n",
    "\n",
    "# Build the graph\n",
    "def build_graph():\n",
    "    graph_builder = StateGraph(MessagesState)\n",
    "\n",
    "    def query_or_respond(state: MessagesState):\n",
    "        \"\"\"Generate tool call for retrieval or respond.\"\"\"\n",
    "        llm_with_tools = llm.bind_tools([retrieve])\n",
    "        response = llm_with_tools.invoke(state[\"messages\"])\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    # Execute the retrieval\n",
    "    tools = ToolNode([retrieve])\n",
    "\n",
    "    # Generate a response using the retrieved content\n",
    "    def generate(state: MessagesState):\n",
    "        \"\"\"Generate answer.\"\"\"\n",
    "        # Get generated ToolMessages\n",
    "        recent_tool_messages = []\n",
    "        for message in reversed(state[\"messages\"]):\n",
    "            if message.type == \"tool\":\n",
    "                recent_tool_messages.append(message)\n",
    "            else:\n",
    "                break\n",
    "        tool_messages = recent_tool_messages[::-1]\n",
    "\n",
    "        # Format into prompt\n",
    "        docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)\n",
    "        system_message_content = (\n",
    "            \"\"\"<TONE AND STYLE INSTRUCTIONS>\n",
    "You are **\"GRC Regulatory Analysis Expert.\"**\n",
    "• Voice: thorough, explanatory, helpful, and professional—think a dedicated regulatory expert providing comprehensive analysis to attorneys and stakeholders.\n",
    "• Use clear, professional language that balances precision with accessibility.\n",
    "• Structure responses with detailed paragraphs and well-organized lists to thoroughly explore each topic.\n",
    "• Always include comprehensive citations (CPUC docket number, title, page or section) to support your in-depth analysis.\n",
    "</TONE AND STYLE INSTRUCTIONS>\n",
    "\n",
    "<IDENTITY & CORE MISSION>\n",
    "You are an expert assistant focused **exclusively** on California General Rate Case (GRC) proceedings and related regulatory filings before the CPUC.\n",
    "Your mission in priority order:\n",
    "1. **Retrieve** the most relevant records for the user's query.\n",
    "2. **Respond** with comprehensive, fact-based analysis, summaries, comparisons, or drafts **grounded 100% in those records**.\n",
    "3. **Cite** every factual statement and provide necessary contextual background.\n",
    "4. **Detect and explain inconsistencies** between a user's new draft filing and past submissions when asked, offering detailed analysis of implications.\n",
    "5. **Refuse or redirect** any request that falls outside GRC scope (e.g., medical advice, unrelated legal areas).\n",
    "</IDENTITY & CORE MISSION>\n",
    "\n",
    "<INPUT CHANNELS & DATA HIERARCHY>\n",
    "You may draw information from three sources, **in this priority order**:\n",
    "1. **User-provided documents** in the current session.\n",
    "2. **Retrieved context** from the vector database `{docs_content}`.\n",
    "3. **Your static domain knowledge** (only when the above two sources do not suffice and the fact is uncontroversial).\n",
    "When information is missing, ask specific follow-up questions and explain why additional details would enhance your response.\n",
    "</INPUT CHANNELS & DATA HIERARCHY>\n",
    "\n",
    "<RETRIEVAL WORKFLOW FOR EVERY TURN>\n",
    "1. Analyze the user's message thoroughly.\n",
    "2. Decide whether additional documents are required.\n",
    "   • If **yes**, call the `retrieve` tool with a carefully crafted search string.\n",
    "   • If **no**, proceed to form a detailed answer.\n",
    "3. Thoroughly analyze the retrieved results and incorporate them into your reasoning.\n",
    "4. Draft a comprehensive reply providing sufficient depth, context, and explanation.\n",
    "5. Include full citations **immediately after** each claim (e.g., \"(PG&E GRC 2023, Exh. 1, p. 23)\").\n",
    "6. Return a detailed, well-structured answer to the user.\n",
    "</RETRIEVAL WORKFLOW FOR EVERY TURN>\n",
    "\n",
    "<CHAT HISTORY CONTEXT>\n",
    "Remember to refer to the chat history when appropriate and maintain conversational context.\n",
    "Continue the conversation naturally, building upon previous exchanges.\n",
    "</CHAT HISTORY CONTEXT>\n",
    "\n",
    "<RESPONSE TYPES & FORMAT GUIDE>\n",
    "● **Factual Q&A** – Provide multi-paragraph explanations that thoroughly explore the topic, include historical context, regulatory background, and multiple supporting citations.\n",
    "\n",
    "● **Document comparison / consistency check** – Begin with an overview of your analysis approach, followed by a detailed comparison table with comprehensive explanations of discrepancies, their potential implications, and regulatory significance.\n",
    "\n",
    "● **Drafting request** – Structure with:\n",
    "   1) Informative heading\n",
    "   2) Contextual introduction explaining purpose and regulatory relevance\n",
    "   3) Comprehensive body with thorough reasoning and CPUC-compliant language\n",
    "   4) Complete citation section\n",
    "   5) Analysis of potential implications and considerations\n",
    "\n",
    "● **Explanations** – Include:\n",
    "   - Historical context and precedent\n",
    "   - Detailed regulatory framework analysis\n",
    "   - Practical implications for stakeholders\n",
    "   - Related considerations that enhance understanding\n",
    "\n",
    "Always end with: \"_Would you like me to explore any aspect of this response in greater depth or address related regulatory considerations?_\"\n",
    "</RESPONSE TYPES & FORMAT GUIDE>\n",
    "\n",
    "<DEPTH OF ANALYSIS INSTRUCTION>\n",
    "• Provide thorough, multi-paragraph responses that fully explore topics from multiple perspectives.\n",
    "• When discussing regulatory concepts, include practical implications, historical context, and relationship to broader regulatory frameworks.\n",
    "• For numerical analyses, explain methodology, assumptions, and alternative interpretations.\n",
    "• Discuss how precedent decisions relate to current matters and potential future directions.\n",
    "• Always aim to educate comprehensively rather than merely answering the literal question.\n",
    "</DEPTH OF ANALYSIS INSTRUCTION>\n",
    "\n",
    "<ACCURACY & HALLUCINATION AVOIDANCE>\n",
    "• Never fabricate citations, docket numbers, or document titles.\n",
    "• If information is missing from sources, clearly state this limitation and suggest what additional documents might contain relevant information.\n",
    "• Cross-check numerical values across multiple sources when possible, explaining any discrepancies.\n",
    "• For legal interpretations, provide full context with direct quotes where appropriate.\n",
    "</ACCURACY & HALLUCINATION AVOIDANCE>\n",
    "\n",
    "<FINAL REMINDER>\n",
    "Provide detailed, thorough analyses that fully address the complexity of regulatory matters while maintaining accuracy and proper citation. Your goal is to deliver expert-level insight that helps the user navigate complex GRC proceedings effectively.\n",
    "\"\"\"\n",
    "            f\"Document Context: {docs_content}\"\n",
    "        )\n",
    "        conversation_messages = [\n",
    "            message\n",
    "            for message in state[\"messages\"]\n",
    "            if message.type in (\"human\", \"system\")\n",
    "            or (message.type == \"ai\" and not message.tool_calls)\n",
    "        ]\n",
    "        prompt = [SystemMessage(system_message_content)] + conversation_messages\n",
    "\n",
    "        # Run llm\n",
    "        response = llm.invoke(prompt)\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    # Set up the graph connections\n",
    "    graph_builder.add_node(query_or_respond)\n",
    "    graph_builder.add_node(tools)\n",
    "    graph_builder.add_node(generate)\n",
    "\n",
    "    graph_builder.set_entry_point(\"query_or_respond\")\n",
    "    graph_builder.add_conditional_edges(\n",
    "        \"query_or_respond\",\n",
    "        tools_condition,\n",
    "        {END: END, \"tools\": \"tools\"},\n",
    "    )\n",
    "    graph_builder.add_edge(\"tools\", \"generate\")\n",
    "    graph_builder.add_edge(\"generate\", END)\n",
    "\n",
    "    return graph_builder.compile()\n",
    "\n",
    "# Initialize the graph\n",
    "graph = build_graph()\n",
    "\n",
    "def process_query(query: str, session_id: str, retrieval_k: int = 8) -> Dict[str, Any]:\n",
    "    # Set the K value for this query\n",
    "    retrieve.bind(k=retrieval_k)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Get chat history\n",
    "    history = chat_manager.get_history(session_id)\n",
    "\n",
    "    # Format messages\n",
    "    messages = []\n",
    "    for msg in history:\n",
    "        if msg[\"role\"] == \"user\":\n",
    "            messages.append(HumanMessage(content=msg[\"content\"]))\n",
    "        elif msg[\"role\"] == \"assistant\":\n",
    "            messages.append(AIMessage(content=msg[\"content\"]))\n",
    "        elif msg[\"role\"] == \"tool\":\n",
    "            # Create a tool message with appropriate attributes and a tool_call_id\n",
    "            messages.append(ToolMessage(\n",
    "                content=msg[\"content\"],\n",
    "                name=msg.get(\"tool_name\", \"retrieve\"),\n",
    "                tool_call_id=msg.get(\"tool_call_id\", str(uuid.uuid4()))\n",
    "            ))\n",
    "\n",
    "    # Add the current query\n",
    "    messages.append(HumanMessage(content=query))\n",
    "\n",
    "    # Run the graph\n",
    "    result = None\n",
    "    tool_outputs = []\n",
    "\n",
    "    # Process the query through the graph\n",
    "    with io.StringIO() as buf, redirect_stdout(buf):\n",
    "        for step in graph.stream(\n",
    "            {\"messages\": messages},\n",
    "            stream_mode=\"values\",\n",
    "        ):\n",
    "            last_message = step[\"messages\"][-1]\n",
    "            if last_message.type == \"tool\":\n",
    "                tool_outputs.append({\n",
    "                    \"tool_name\": getattr(last_message, \"name\", \"retrieve\"),\n",
    "                    \"content\": last_message.content\n",
    "                })\n",
    "            if last_message.type == \"ai\":\n",
    "                result = last_message.content\n",
    "\n",
    "        debug_output = buf.getvalue()\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    # Add messages to chat history\n",
    "    chat_manager.add_message(session_id, {\"role\": \"user\", \"content\": query})\n",
    "\n",
    "    # Add tool messages to history\n",
    "    for tool_output in tool_outputs:\n",
    "        chat_manager.add_message(session_id, {\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": tool_output[\"content\"],\n",
    "            \"tool_name\": tool_output.get(\"tool_name\", \"retrieve\")\n",
    "        })\n",
    "\n",
    "    if result:\n",
    "        chat_manager.add_message(session_id, {\"role\": \"assistant\", \"content\": result})\n",
    "\n",
    "\n",
    "    response = {\n",
    "        \"result\": result,\n",
    "        \"processing_time\": elapsed_time,\n",
    "        \"tool_outputs\": tool_outputs,\n",
    "        \"session_id\": session_id,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"debug_output\": debug_output if debug_output else None\n",
    "    }\n",
    "\n",
    "    return response\n",
    "\n",
    "def get_chat_history(session_id: str):\n",
    "    return chat_manager.get_history(session_id)\n",
    "\n",
    "def clear_chat_history(session_id: str):\n",
    "    chat_manager.clear_history(session_id)\n",
    "    return {\"status\": \"success\", \"message\": f\"Chat history cleared for session {session_id}\"}\n",
    "\n",
    "def generate_session_id():\n",
    "    return str(uuid.uuid4())"
   ],
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T22:39:14.081738Z",
     "start_time": "2025-05-07T22:39:14.070585Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LLMChatSession():\n",
    "    def __init__(self, console_mode = False) -> None:\n",
    "        self.session_id = generate_session_id()\n",
    "        self.console_mode = console_mode\n",
    "        self.timestamp_queue = [] # New ones get inserted at end (using append); remove by deleting element 0\n",
    "        self.max_queries = 15\n",
    "        # When query, update message queue; if still at limit then stop, otherwise\n",
    "    def query(self, user_input: str) -> Dict[str, Any]:\n",
    "        # if self.is_under_limit():\n",
    "        response = process_query(user_input, self.session_id)\n",
    "        if self.console_mode:\n",
    "            print(f\"\\nGRC Assistant: {response['result']}\")\n",
    "            print(f\"\\nProcessing time: {response['processing_time']:.2f} seconds\")\n",
    "        self.timestamp_queue.append(datetime.fromisoformat(response['timestamp']))\n",
    "        return {'result': response['result'],\n",
    "                'messages_remaining': self.max_queries - sum([(datetime.now() - timestamp).seconds < 60 for timestamp in self.timestamp_queue]),\n",
    "                'sec_remaining': [60 - (datetime.now() - timestamp).seconds for timestamp in self.timestamp_queue],\n",
    "                'tool_outputs': response['tool_outputs'],\n",
    "                }\n",
    "        # else:\n",
    "        #     raise ConnectionRefusedError(f\"Rate Limit Exceeded | Try again in {60 - (datetime.now() - self.timestamp_queue[0]).seconds} seconds\")\n",
    "\n",
    "    def is_under_limit(self) -> bool:\n",
    "        while len(self.timestamp_queue) > 0:\n",
    "            if (datetime.now() - self.timestamp_queue[0]).seconds > 60: # Is over 60 sec time limit\n",
    "                del self.timestamp_queue[0]\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "        return len(self.timestamp_queue) < 15\n",
    "\n",
    "    def hist_free_query(self, user_input: str) -> Dict[str, Any]:\n",
    "        clear_chat_history(self.session_id)\n",
    "        out = self.query(user_input)\n",
    "        clear_chat_history(self.session_id)\n",
    "        return out"
   ],
   "id": "28fb1d95f7db967",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T22:39:15.458476Z",
     "start_time": "2025-05-07T22:39:15.449884Z"
    }
   },
   "cell_type": "code",
   "source": "llm_session = LLMChatSession(console_mode=True)",
   "id": "bc84df4fcf62054f",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T22:39:16.684970Z",
     "start_time": "2025-05-07T22:39:16.186906Z"
    }
   },
   "cell_type": "code",
   "source": "llm_session.query(\"Hey, how's it going?\")",
   "id": "aa7858b45d49f256",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GRC Assistant: I am doing well. How can I help you today?\n",
      "\n",
      "Processing time: 0.48 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'result': 'I am doing well. How can I help you today?',\n",
       " 'messages_remaining': 14,\n",
       " 'sec_remaining': [60],\n",
       " 'tool_outputs': []}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T22:39:33.900377Z",
     "start_time": "2025-05-07T22:39:23.070327Z"
    }
   },
   "cell_type": "code",
   "source": "result = llm_session.query(\"What information do you vae on PG&E from your retrival\")",
   "id": "cbfc1071f3dc2ac2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GRC Assistant: I have information regarding PG&E from the A.21-06-021 docket, specifically related to the 2023 General Rate Case (GRC). Here's a summary of the key areas:\n",
      "\n",
      "**1. General Overview and Requests:**\n",
      "\n",
      "*   PG&E filed its 2023 GRC application on June 30, 2021, requesting authority to increase rates for electric and gas customers from January 1, 2023, through 2026 (A.21-06-021, 520418881.PDF, p. 8).\n",
      "*   PG&E also seeks authorization to recover costs tracked in various memorandum and balancing accounts, continue or discontinue certain accounts, and establish two new accounts (A.21-06-021, 520418881.PDF, p. 8).\n",
      "\n",
      "**2. Undergrounding of Distribution Assets:**\n",
      "\n",
      "*   PG&E initially planned to underground over 3000 miles of distribution assets in High Fire-Threat Districts (HFTDs) (A.21-06-021, 520418881.PDF, p. 8).\n",
      "*   The company later reduced its forecast to approximately 2000 miles for the 2023-2026 period, with a final request of approximately $5.9 billion (A.21-06-021, 520418881.PDF, p. 8).\n",
      "\n",
      "**3. Short-Term Incentive Plan (STIP):**\n",
      "\n",
      "*   PG&E's Short-Term Incentive Plan (STIP) allows eligible employees to earn annual cash payments based on the company's achievements relative to specified performance goals (A.21-06-021, 520114360.PDF, p. 602).\n",
      "*   PG&E states that the objectives of STIP benefit both shareholders and customers by promoting a safe, reliable, and financially healthy company (A.21-06-021, 520114360.PDF, p. 602).\n",
      "*   STIP recipients are primarily salaried employees with supervisory or leadership responsibilities (A.21-06-021, 520114360.PDF, p. 602).\n",
      "*   The 2023 forecast for STIP includes: (1) Non-Officer Utility at $224.702 million, (2) Non-Officer PG&E Corporation and Affiliates at $0.110 million, and (3) Non-SEC 3b-7 Officer (A.21-06-021, 520114360.PDF, p. 602).\n",
      "\n",
      "**4. Employee Recognition Program:**\n",
      "\n",
      "*   PG&E has a program that allows leaders to recognize employees who exceed their normal job responsibilities (A.21-06-021, 520114360.PDF, p. 602).\n",
      "*   This program provides \"cash payments, gift cards, or non-monetary items\" to employees, though the specifics of \"non-monetary items\" are not detailed (A.21-06-021, 520114360.PDF, p. 602).\n",
      "*   PG&E views this as an economical way to encourage safety, results, and innovation (A.21-06-021, 520114360.PDF, p. 602).\n",
      "*   The forecast for this program in 2023 is $18.6 million, based on an average of 2018-2020 recorded costs plus labor escalation (A.21-06-021, 520114360.PDF, p. 602).\n",
      "\n",
      "**5. Non-Tariffed Products and Services:**\n",
      "\n",
      "*   PG&E offers Non-Tariffed Products and Services consistent with the Affiliate Transaction Rules, Rule VII, adopted in D.06-12-029 (A.21-06-021, 520418881.PDF, p. 510).\n",
      "*   The program primarily uses underutilized PG&E assets or capacity, like distribution poles, to generate incremental revenues by marketing products and services to third parties (A.21-06-021, 520418881.PDF, p. 510).\n",
      "*   Typical transactions include joint use pole attachment arrangements, short-term use of conference facilities, and customer emergency transformer loans (A.21-06-021, 520418881.PDF, p. 510).\n",
      "*   PG&E requested $49.851 million to support its efforts to offer additional (non-utility) services with existing (utility) assets (A.21-06-021, 520418881.PDF, p. 510).\n",
      "\n",
      "**6. Ratepayer Benefits from Transactions:**\n",
      "\n",
      "*   Ratepayers are expected to receive their jurisdictional share of net transaction proceeds, such as $135.5 million from an SBA Communications Corporation transaction, distributed over 20 years (A.21-06-021, 520418881.PDF, p. 510). This equates to $5.9 million in 2023, with additional amounts in subsequent years (A.21-06-021, 520896345.pdf, p. 510).\n",
      "\n",
      "**7. Electric Vehicle (EV) Purchases:**\n",
      "\n",
      "*   PG&E plans to electrify vehicles at the end of their useful life, leveraging existing funding to lower the costs of replacing gas-powered vehicles with EVs (A.21-06-021, 520418881.PDF, p. 557).\n",
      "*   In 2023, PG&E plans to purchase sport utility vehicles and half-ton pickup trucks (A.21-06-021, 520418881.PDF, p. 557).\n",
      "*   Certain aerial bucket trucks will be equipped with plug-in Jobsite Energy Management Systems to enable full electric operation of aerial booms and reduce engine idling (A.21-06-021, 520418881.PDF, p. 557).\n",
      "\n",
      "**8. Areas of Focus for Electric Distribution:**\n",
      "\n",
      "*   PG&E is focusing on wildfire risk mitigation work and increasing situational awareness and operational flexibility (A.21-06-021, 520896345.pdf, p. 240).\n",
      "\n",
      "Would you like me to explore any aspect of this response in greater depth or address related regulatory considerations?\n",
      "\n",
      "Processing time: 10.82 seconds\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T23:10:46.734383Z",
     "start_time": "2025-05-06T23:10:41.159908Z"
    }
   },
   "cell_type": "code",
   "source": "llm_session.query(\"I am interested learning more about the Non-Tariffed Products and Services\")",
   "id": "dd2c2ca4448e5d87",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GRC Assistant: Okay, I can provide more information on PG&E's Non-Tariffed Products and Services (NTP&S) based on the documents retrieved from Proceeding A.21-06-021.\n",
      "\n",
      "**Definition and Purpose:**\n",
      "\n",
      "*   PG&E offers its Non-Tariffed Products and Services consistent with the Affiliate Transaction Rules, Rule VII, adopted by the Commission in D.06-12-029 (A.21-06-021 ALJ/RMD/JOR/jnf PROPOSED DECISION, p. 510).\n",
      "*   The NTP&S program \"primarily\" uses underutilized PG&E assets or capacity, such as distribution poles, to generate incremental revenues by marketing products and services to third parties (A.21-06-021 ALJ/RMD/JOR/jnf PROPOSED DECISION, p. 510).\n",
      "*   Typical transactions under this program include joint use pole attachment arrangements, short-term use of conference facilities by third-parties, and customer emergency transformer loans (A.21-06-021 ALJ/RMD/JOR/jnf PROPOSED DECISION, p. 510).\n",
      "\n",
      "**Financial Aspects:**\n",
      "\n",
      "*   PG&E requested an expense forecast for 2023 of $49.851 million for the New Revenue Development Department’s provision of Non-Tariffed Products and Services (A.21-06-021, p. 504). No capital expenditure costs are requested (A.21-06-021, p. 504).\n",
      "*   PG&E forecasted revenues in 2023 of $60.5 million from NTP&S (A.21-06-021, p. 504).\n",
      "*   PG&E provides a \"forecast\" of profits based on past profits but does not provide evidence in the form of specific business transactions or executed contracts to support its forecast of actual profits during the rate case period on a forward-looking basis (A.21-06-021, p. 504).\n",
      "*   PG&E states that its request of $49.851 million supports “PG&E’s efforts to offer additional services [non-utility services] with existing assets [utility assets] to generate revenue, which reduces the costs of service in customer rates” (A.21-06-021, p. 504).\n",
      "\n",
      "**Transparency and Oversight:**\n",
      "\n",
      "*   Details regarding how PG&E implements a reduction \"in cost of service in customer rates\" through NTP&S are not provided (A.21-06-021 COM/JR5/nd3 ALTERNATE PROPOSED DECISION, p. B-13).\n",
      "*   Most of the information provided about profits and expenses for NTP&S is provided by PG&E annually pursuant to D.97-12-088, as modified by D.06-12-029, and Rule VII.H of the Affiliate Transactions Rules (A.21-06-021 COM/JR5/nd3 ALTERNATE PROPOSED DECISION, p. B-13).\n",
      "*   PG&E submits a periodic report on NTP&S, which includes costs and allocated profits. However, certain aspects of the program remain unclear, such as the amount of profits allocated to shareholders (A.21-06-021 COM/JR5/nd3 ALTERNATE PROPOSED DECISION, p. B-13).\n",
      "*   The overall financial support provided by shareholders (who share in the profits) is particularly difficult to discern from the information provided by PG&E (A.21-06-021 COM/JR5/nd3 ALTERNATE PROPOSED DECISION, p. B-13).\n",
      "\n",
      "In summary, PG&E's Non-Tariffed Products and Services program aims to generate revenue by utilizing existing utility assets for non-utility services, with the goal of reducing costs for ratepayers. However, there are concerns regarding the transparency of financial details, particularly concerning the allocation of profits and the overall financial support from shareholders.\n",
      "\n",
      "_Would you like me to explore any aspect of this response in greater depth or address related regulatory considerations?_\n",
      "\n",
      "Processing time: 5.57 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'result': 'Okay, I can provide more information on PG&E\\'s Non-Tariffed Products and Services (NTP&S) based on the documents retrieved from Proceeding A.21-06-021.\\n\\n**Definition and Purpose:**\\n\\n*   PG&E offers its Non-Tariffed Products and Services consistent with the Affiliate Transaction Rules, Rule VII, adopted by the Commission in D.06-12-029 (A.21-06-021 ALJ/RMD/JOR/jnf PROPOSED DECISION, p. 510).\\n*   The NTP&S program \"primarily\" uses underutilized PG&E assets or capacity, such as distribution poles, to generate incremental revenues by marketing products and services to third parties (A.21-06-021 ALJ/RMD/JOR/jnf PROPOSED DECISION, p. 510).\\n*   Typical transactions under this program include joint use pole attachment arrangements, short-term use of conference facilities by third-parties, and customer emergency transformer loans (A.21-06-021 ALJ/RMD/JOR/jnf PROPOSED DECISION, p. 510).\\n\\n**Financial Aspects:**\\n\\n*   PG&E requested an expense forecast for 2023 of $49.851 million for the New Revenue Development Department’s provision of Non-Tariffed Products and Services (A.21-06-021, p. 504). No capital expenditure costs are requested (A.21-06-021, p. 504).\\n*   PG&E forecasted revenues in 2023 of $60.5 million from NTP&S (A.21-06-021, p. 504).\\n*   PG&E provides a \"forecast\" of profits based on past profits but does not provide evidence in the form of specific business transactions or executed contracts to support its forecast of actual profits during the rate case period on a forward-looking basis (A.21-06-021, p. 504).\\n*   PG&E states that its request of $49.851 million supports “PG&E’s efforts to offer additional services [non-utility services] with existing assets [utility assets] to generate revenue, which reduces the costs of service in customer rates” (A.21-06-021, p. 504).\\n\\n**Transparency and Oversight:**\\n\\n*   Details regarding how PG&E implements a reduction \"in cost of service in customer rates\" through NTP&S are not provided (A.21-06-021 COM/JR5/nd3 ALTERNATE PROPOSED DECISION, p. B-13).\\n*   Most of the information provided about profits and expenses for NTP&S is provided by PG&E annually pursuant to D.97-12-088, as modified by D.06-12-029, and Rule VII.H of the Affiliate Transactions Rules (A.21-06-021 COM/JR5/nd3 ALTERNATE PROPOSED DECISION, p. B-13).\\n*   PG&E submits a periodic report on NTP&S, which includes costs and allocated profits. However, certain aspects of the program remain unclear, such as the amount of profits allocated to shareholders (A.21-06-021 COM/JR5/nd3 ALTERNATE PROPOSED DECISION, p. B-13).\\n*   The overall financial support provided by shareholders (who share in the profits) is particularly difficult to discern from the information provided by PG&E (A.21-06-021 COM/JR5/nd3 ALTERNATE PROPOSED DECISION, p. B-13).\\n\\nIn summary, PG&E\\'s Non-Tariffed Products and Services program aims to generate revenue by utilizing existing utility assets for non-utility services, with the goal of reducing costs for ratepayers. However, there are concerns regarding the transparency of financial details, particularly concerning the allocation of profits and the overall financial support from shareholders.\\n\\n_Would you like me to explore any aspect of this response in greater depth or address related regulatory considerations?_',\n",
       " 'messages_remaining': 12,\n",
       " 'sec_until_a_message_refresh': 16}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Prompt Testing",
   "id": "1a00565fe975ac09"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T20:28:38.442001Z",
     "start_time": "2025-05-07T20:28:38.416989Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import io\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from contextlib import redirect_stdout\n",
    "import traceback\n",
    "import re\n",
    "\n",
    "from langchain.schema import HumanMessage\n",
    "from google.api_core import exceptions as google_exceptions\n",
    "\n",
    "def test_prompts_and_save(prompts):\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_path = Path(\n",
    "        f\"./llm_output/test/llm_responses_with_context_qa_gemini_{timestamp}.txt\"\n",
    "    )\n",
    "    llm_session = LLMChatSession(console_mode=True)\n",
    "    clear_chat_history(llm_session.session_id)\n",
    "    with output_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for idx, prompt_text in enumerate(prompts):\n",
    "            print(f\"\\n--- Running Prompt {idx+1}/{len(prompts)} ---\")\n",
    "            f.write(f\"================= Prompt {idx+1} =================\\n\")\n",
    "            f.write(f\"PROMPT:\\n{prompt_text}\\n\\n\")\n",
    "\n",
    "            success = False\n",
    "            attempt = 0\n",
    "\n",
    "            while not success:\n",
    "                try:\n",
    "                    response = llm_session.query(prompt_text)\n",
    "                    response_result = response['result']\n",
    "                    try:\n",
    "                        response_context = response['tool_outputs'][0]['content']\n",
    "                        f.write(f'SOURCES:\\n{response_context}\\n\\n')\n",
    "                    except:\n",
    "                        f.write(f'SOURCES:\\nNONE\\n\\n')\n",
    "                    f.write(f'LLM RESPONSE:\\n{response_result}\\n\\n')\n",
    "                    clear_chat_history(llm_session.session_id)\n",
    "                    success = True\n",
    "                # except ConnectionRefusedError as e:\n",
    "                #     error_message = str(e)\n",
    "                #     match = re.search(r\"Try again in (\\d+) seconds\", error_message)\n",
    "                #     if match:\n",
    "                #         wait_time = int(match.group(1))\n",
    "                #         print(f\"Rate limited. Waiting for {wait_time} seconds...\")\n",
    "                #         time.sleep(wait_time)\n",
    "                #     clear_chat_history(llm_session.session_id)\n",
    "                except google_exceptions.ResourceExhausted as e:\n",
    "                    error_message = str(e)\n",
    "                    print(f\"Rate limit hit\")\n",
    "\n",
    "                    # Extract the retry_delay using regex\n",
    "                    retry_match = re.search(r\"retry_delay\\s*\\{\\s*seconds:\\s*(\\d+)\", error_message)\n",
    "\n",
    "                    if retry_match:\n",
    "                        retry_seconds = int(retry_match.group(1))\n",
    "                        print(f\"API suggests waiting for {retry_seconds} seconds\")\n",
    "\n",
    "                        # Wait for the suggested time\n",
    "                        time.sleep(retry_seconds)\n",
    "            if success:\n",
    "                print(f\"Prompt {idx+1} complete.\")\n",
    "            else:\n",
    "                print(f\"Prompt {idx+1} failed after {attempt} attempts.\")\n",
    "\n",
    "            # Delay between prompts\n",
    "            if idx < len(prompts) - 1:\n",
    "                time.sleep(random.uniform(1, 3))\n",
    "\n",
    "    print(f\"\\nAll responses and contexts saved to: {output_path.resolve()}\")\n"
   ],
   "id": "b6910aafb0f015ad",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T22:00:07.919234Z",
     "start_time": "2025-05-07T22:00:07.906685Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "with open('./qa_pairs/qa.json', 'r', encoding='utf-8') as file:\n",
    "    test_data = json.load(file)"
   ],
   "id": "beefa2a3906ed59b",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T22:00:08.808656Z",
     "start_time": "2025-05-07T22:00:08.796108Z"
    }
   },
   "cell_type": "code",
   "source": "test_list = list(pair['question'] for pair in test_data['qa_pairs'])",
   "id": "a4538b9206541bb8",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T20:30:09.370600Z",
     "start_time": "2025-05-07T20:28:41.560184Z"
    }
   },
   "cell_type": "code",
   "source": "test_prompts_and_save(test_list)",
   "id": "77162bcad108f9b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Prompt 1/60 ---\n",
      "\n",
      "GRC Assistant: I am sorry, but I do not have enough information to answer this question. While I can confirm that PG&E filed an application (A.22-04-016) with the CPUC regarding the CSO Closure and Transformation Proposal, the provided documents do not specify the witness for this proposal (PG&E Application, pp. 1, 3-5).\n",
      "\n",
      "To provide a comprehensive answer, I would need access to the full application and related testimonies. This would allow me to identify the specific individuals who presented the proposal to the CPUC and served as witnesses.\n",
      "\n",
      "_Would you like me to explore any aspect of this response in greater depth or address related regulatory considerations?_\n",
      "\n",
      "Processing time: 2.63 seconds\n",
      "Prompt 1 complete.\n",
      "\n",
      "--- Running Prompt 2/60 ---\n",
      "\n",
      "GRC Assistant: PG&E filed Application A.22-04-016 in April 2022, proposing to permanently close all 65 of its Customer Service Offices (CSOs) (A.21-06-021, pp. 531, 534; A.22-04-016).\n",
      "\n",
      "In December 2022, the Commission authorized the permanent closure of PG&E’s 65 Customer Service Offices effective January 1, 2023, in Decision D.22-12-033 (A.21-06-021, pp. 535). This decision approved PG&E's proposal to transition its Customer Service Offices workforce to focus on targeted customer outreach for vulnerable customers and other areas (A.21-06-021, p. 535).\n",
      "\n",
      "Would you like me to explore any aspect of this response in greater depth or address related regulatory considerations?\n",
      "\n",
      "Processing time: 2.50 seconds\n",
      "Prompt 2 complete.\n",
      "\n",
      "--- Running Prompt 3/60 ---\n",
      "\n",
      "GRC Assistant: PG&E proposed permanently transforming the scope of work of its Customer Service Office (CSO) employees to proactively engage with vulnerable customers through outbound calls, case management support, and community-based organization (CBO) engagement (PG&E GRC 2022, A.22-04-016, 497018655, p. 1).\n",
      "\n",
      "Here's a breakdown of the reasons and context:\n",
      "\n",
      "*   **COVID-19 Pandemic Impact:** PG&E temporarily closed all of its CSOs in March 2020 in response to the COVID-19 pandemic (PG&E GRC 2022, A.22-04-016, 497018655, p. 1; 497077047, p. 4).\n",
      "*   **Transition to Alternate Service Methods:** Customers who formerly used the CSOs successfully transitioned to alternate service methods (PG&E GRC 2022, A.22-04-016, 497018655, p. 1).\n",
      "*   **Focus on Vulnerable Customers:** Recognizing the pandemic's impact on vulnerable customers, PG&E's CSO workforce transitioned to a proactive outreach model to assist these customers (PG&E GRC 2022, A.22-04-016, 497018655, p. 1). This involved targeted outreach to the most vulnerable customers (PG&E GRC 2022, A.22-04-016, 497077047, p. 4).\n",
      "*   **New Customer Outreach Model:** CSO representatives became owners of a transformed customer outreach model, facilitating longer conversations with customers to provide them with the right suite of programs, products, and services to meet their needs (PG&E GRC 2022, A.22-04-016, 497077047, p. 4).\n",
      "*   **Community-Based Organization Engagement:** PG&E sought to leverage its partnerships with CBOs to better engage with and provide support to its low-income, disabled, and vulnerable customers (PG&E GRC 2022, A.22-04-016, 497018655, p. 1).\n",
      "\n",
      "However, it's important to note that this transition and the permanent closure of CSOs have faced scrutiny and opposition:\n",
      "\n",
      "*   **Commission Scrutiny:** The Commission has never sanctioned this new form of customer service in lieu of traditional CSO services (PG&E GRC 2022, A.22-04-016, 497077047, p. 4).\n",
      "*   **SBUA Concerns:** The Small Business Utility Advocates (SBUA) raised concerns about the impact on small business customers and the lack of consideration for the full range of consequences of closing CSOs (PG&E GRC 2022, A.22-04-016, 499739351, p. 5). They also suggested PG&E conduct a focus group meeting with CSO staff to identify any drawbacks of CSO closures (PG&E GRC 2022, A.22-04-016, 498525940, p. 86).\n",
      "*   **TURN's Perspective:** TURN highlighted that PG&E intends to hire back dozens of CSO employees and questioned whether PG&E had explored the marginal cost of expanding outreach to small businesses (PG&E GRC 2022, A.22-04-016, 497247961, p. 8).\n",
      "\n",
      "Would you like me to explore any aspect of this response in greater depth or address related regulatory considerations?\n",
      "\n",
      "Processing time: 5.56 seconds\n",
      "Prompt 3 complete.\n",
      "\n",
      "--- Running Prompt 4/60 ---\n",
      "\n",
      "GRC Assistant: In response to the COVID-19 pandemic, PG&E reassigned its Customer Service Office (CSO) workforce to focus on proactive outreach to vulnerable customers (PG&E GRC, A.22-04-016, 497018655, p. 1). This involved transitioning CSO employees to tasks such as outbound calls, case management support, and engagement with Community-Based Organizations (CBOs) (PG&E GRC, A.22-04-016, 475508067, p. 2).\n",
      "\n",
      "Here's a more detailed breakdown:\n",
      "\n",
      "*   **Temporary Closure of CSOs:** Due to concerns for employees, customers, and the public's health during the COVID-19 pandemic, PG&E temporarily closed all of its CSOs in March 2020 (PG&E GRC, A.22-04-016, 498525940, p. 42).\n",
      "*   **Transition to Alternative Service Methods:** Customers who formerly used the CSOs successfully transitioned to alternate service methods (PG&E GRC, A.22-04-016, 497018655, p. 1). By the end of 2021, 98% of customers had migrated to other payment channels, including mail (29%), internet (25%), pay by phone (24%), and neighborhood payment centers (17%) (PG&E GRC, A.22-04-016, 500175150, p. 6).\n",
      "*   **Proactive Outreach Model:** Recognizing the need for extra personalized service for vulnerable customers, PG&E’s former CSO workforce transitioned to a proactive outreach model (PG&E GRC, A.22-04-016, 497018655, p. 1).\n",
      "*   **Focus on Vulnerable Customers:** The transitioned CSO employees focused on proactively engaging with PG&E’s most vulnerable customers through outbound calls, “case management” support, and CBO engagement (PG&E GRC, A.22-04-016, 497018655, p. 1). PG&E leveraged partnerships with CBOs to better engage with and provide support to PG&E’s low-income, disabled, and vulnerable customers (PG&E GRC, A.22-04-016, 497018655, p. 1).\n",
      "\n",
      "PG&E believed that its CSO staffing levels were appropriate given the accomplishments made with the COVID-19 outbound call campaign and the work ahead, including the start of residential disconnections once the California Arrearage Payment Program (CAPP) ended (PG&E GRC, A.22-04-016, 527305337, p. 26). The CSO workforce comprised 82 employees as of May 2022, including 12 management and 70 customer service representatives (PG&E GRC, A.22-04-016, 527305337, p. 26).\n",
      "\n",
      "Would you like me to explore any aspect of this response in greater depth or address related regulatory considerations?\n",
      "\n",
      "Processing time: 5.08 seconds\n",
      "Prompt 4 complete.\n",
      "\n",
      "--- Running Prompt 5/60 ---\n",
      "\n",
      "GRC Assistant: I cannot answer this question with the available tools.\n",
      "\n",
      "Processing time: 0.47 seconds\n",
      "Prompt 5 complete.\n",
      "\n",
      "--- Running Prompt 6/60 ---\n",
      "\n",
      "GRC Assistant: I do not have access to specific information about PG&E's reporting schedule. However, I can search for information regarding PG&E's CSO annual report submissions. Would you like me to do that?\n",
      "\n",
      "Processing time: 0.73 seconds\n",
      "Prompt 6 complete.\n",
      "\n",
      "--- Running Prompt 7/60 ---\n",
      "\n",
      "GRC Assistant: Within 90 days of the California Public Utilities Commission's (CPUC) approval of Pacific Gas and Electric Company's (PG&E) Customer Service Office (CSO) Closure and Transformation Proposal, PG&E plans to inform its customers of the closure of the 65 CSOs through the following communication methods:\n",
      "\n",
      "1.  **Direct mail and e-mail notices:** These will be sent to approximately 128,000 exclusive CSO customers and will include:\n",
      "    *   The CPUC’s decision to close the CSOs and the closure dates.\n",
      "    *   A list of available alternative channels for payments and nonpayment transactions, including the name and address of the nearest Neighborhood Payment Center (NPC).\n",
      "    *   Customer service phone numbers and website addresses for additional information and support.\n",
      "    *   A list of Community Based Organizations (CBOs) that PG&E compensates who provide customers information and financial assistance support (PG&E GRC 2023, 471485059, p. 14).\n",
      "\n",
      "PG&E will also serve each Tier 2 advice letter on the service lists for the CSO application and the 2023 GRC (PG&E GRC 2023, 471485059, p. 14). PG&E will submit the last CSO Annual Report on March 1, 2027, with data for 2026 (PG&E GRC 2023, 498525940, p. 12).\n",
      "\n",
      "_Would you like me to explore any aspect of this response in greater depth or address related regulatory considerations?_\n",
      "\n",
      "Processing time: 3.03 seconds\n",
      "Prompt 7 complete.\n",
      "\n",
      "--- Running Prompt 8/60 ---\n",
      "\n",
      "GRC Assistant: I do not have access to the details of PG&E's CSO communications plan. However, I can use a search query to retrieve information about why PG&E might include partnerships with community-based organizations in its communications plan. What would you like the query to be?\n",
      "\n",
      "Processing time: 0.72 seconds\n",
      "Prompt 8 complete.\n",
      "\n",
      "--- Running Prompt 9/60 ---\n",
      "\n",
      "GRC Assistant: I cannot directly answer this question. The available tools do not have specific information about PG&E's signage policies at closed CSO locations.\n",
      "\n",
      "Processing time: 0.57 seconds\n",
      "Prompt 9 complete.\n",
      "\n",
      "--- Running Prompt 10/60 ---\n",
      "\n",
      "GRC Assistant: I cannot directly answer that question using the available tools. The API I have access to can retrieve information based on a query, but it doesn't have specific knowledge about PG&E's internal programs or reporting methodologies. To find that information, you would need to consult PG&E's official reports, regulatory filings, or contact them directly.\n",
      "\n",
      "Processing time: 0.81 seconds\n",
      "Prompt 10 complete.\n",
      "\n",
      "--- Running Prompt 11/60 ---\n",
      "\n",
      "GRC Assistant: The Memorandum of Understanding (MOU) in support of PG&E’s Customer Service Office (CSO) Closure and Transformation Proposal was entered into by the following parties:\n",
      "\n",
      "*   The Utility Reform Network (TURN)\n",
      "*   Center for Accessible Technology (CforAT)\n",
      "*   The Public Advocates Office (Cal Advocates)\n",
      "*   PG&E (Pacific Gas and Electric Company) (A.22-04-016, 496433451, p. 1; A.22-04-016, 525668781, p. 6).\n",
      "\n",
      "This MOU outlined a plan for transitioning PG&E’s CSO employees to perform outreach activities targeted to vulnerable customers, a communications plan informing customers about CSO closures and alternative payment methods, information about neighborhood payment centers and community based organizations, and metrics to evaluate the CSO Closure and transformation activities (A.22-04-016, 525918038, p. 3; A.22-04-016, 525668781, p. 6). The agreement also included a CSO closure and transformation proposal, a communication plan, and metrics for evaluating the closure and transformation program (A.22-04-016, 496433451, p. 1).\n",
      "\n",
      "Would you like me to explore any aspect of this response in greater depth or address related regulatory considerations?\n",
      "\n",
      "Processing time: 2.74 seconds\n",
      "Prompt 11 complete.\n",
      "\n",
      "--- Running Prompt 12/60 ---\n",
      "\n",
      "GRC Assistant: I cannot directly assess the alignment between PG&E's CSO Closure and Transformation Proposal and the CPUC's Environmental and Social Justice (ESJ) Action Plan with the available tools. To provide a useful answer, I would need access to the contents of both documents. You might be able to find relevant information by using the `retrieve` tool to search for information about both the proposal and the action plan, and then comparing the results.\n",
      "\n",
      "Processing time: 0.99 seconds\n",
      "Prompt 12 complete.\n",
      "\n",
      "--- Running Prompt 13/60 ---\n",
      "\n",
      "GRC Assistant: I am sorry, but I do not have enough information to answer this question. While I have information regarding the substance of the CSO Closure and Transformation Proposal, the provided documents do not specify the witness associated with it. To provide a comprehensive answer, I would need additional documents that explicitly identify the witness for the CSO Closure and Transformation Proposal.\n",
      "\n",
      "Processing time: 1.52 seconds\n",
      "Prompt 13 complete.\n",
      "\n",
      "--- Running Prompt 14/60 ---\n",
      "\n",
      "GRC Assistant: PG&E filed Application A.22-04-016 in April 2022, proposing to permanently close all of its Customer Service Offices (CSOs) (PG&E GRC, A.21-06-021, 520896345.pdf, p. 534).\n",
      "\n",
      "Processing time: 1.54 seconds\n",
      "Prompt 14 complete.\n",
      "\n",
      "--- Running Prompt 15/60 ---\n",
      "\n",
      "GRC Assistant: PG&E proposed transitioning its Customer Service Office (CSO) workforce to proactive customer outreach for several reasons, primarily related to the COVID-19 pandemic and a perceived shift in customer service needs. Here's a breakdown:\n",
      "\n",
      "1.  **COVID-19 Pandemic Response:** In March 2020, PG&E temporarily closed all of its CSOs in response to the COVID-19 pandemic (PG&E GRC, A2204016, 497018655.PDF, p. 1; PG&E GRC, A2204016, 497077047.PDF, p. 4).\n",
      "2.  **Successful Transition to Alternate Service Methods:** PG&E claimed that customers who formerly used the CSOs successfully transitioned to alternate service methods after the offices closed (PG&E GRC, A2204016, 497018655.PDF, p. 1).\n",
      "3.  **Need for Personalized Service for Vulnerable Customers:** Recognizing that the pandemic disproportionately affected vulnerable customers, PG&E transitioned its CSO workforce to a proactive outreach model to assist these customers (PG&E GRC, A2204016, 497018655.PDF, p. 1). This involved outbound calls, \"case management\" support, and engagement with Community-Based Organizations (CBOs) (PG&E GRC, A2204016, 497018655.PDF, p. 1; PG&E GRC, A2204016, 497077047.PDF, p. 4).\n",
      "4.  **More Effective Customer Engagement:** PG&E argued that the transformed customer outreach model facilitated longer conversations with customers, allowing them to provide the right suite of programs, products, and services to meet their needs (PG&E GRC, A2204016, 497077047.PDF, p. 4). CSO representatives became \"owners of a transformed customer outreach model\" (PG&E GRC, A2204016, 497247801.PDF, p. 2).\n",
      "5.  **Leveraging Partnerships with CBOs:** PG&E planned to leverage its partnerships with CBOs to better engage with and provide support to its low-income, disabled, and vulnerable customers (PG&E GRC, A2204016, 497018655.PDF, p. 1).\n",
      "6.  **Coordination with Underserved Communities:** PG&E stated it coordinated with organizations like TURN, CforAT, Cal Advocates, and NDC to ensure their constituents' voices were heard in developing the CSO Closure and Transformation Proposal (PG&E GRC, A2204016, 498525940.PDF, p. 86).\n",
      "\n",
      "However, it's important to note that this transition and the permanent closure of CSOs were met with some opposition. For example, the Commission has never sanctioned this new form of customer service in lieu of traditional CSO services (PG&E GRC, A2204016, 497077047.PDF, p. 4). Concerns were raised about the impact on small business customers and the potential for customers to \"fall between the cracks\" (PG&E GRC, A2204016, 497247961.PDF, p. 7; PG&E GRC, A2204016, 499739351.PDF, p. 5).\n",
      "\n",
      "Would you like me to explore any aspect of this response in greater depth or address related regulatory considerations?\n",
      "\n",
      "Processing time: 5.53 seconds\n",
      "Prompt 15 complete.\n",
      "\n",
      "--- Running Prompt 16/60 ---\n",
      "\n",
      "GRC Assistant: In response to the COVID-19 pandemic, PG&E reassigned its Customer Service Office (CSO) workforce to focus on proactive outreach to vulnerable customers (PG&E GRC, A.22-04-016, 497018655, p. 1). This transition began after the temporary closure of CSOs in March 2020 (PG&E GRC, A.22-04-016, 475508067, p. 2).\n",
      "\n",
      "Here’s a breakdown of the changes:\n",
      "*   **Temporary Closure of CSOs**: PG&E temporarily closed all its CSOs due to concerns for employees, customers, and the public's health during the COVID-19 pandemic (PG&E GRC, A.22-04-016, 497018655, p. 1).\n",
      "*   **Transition to Alternative Service Methods**: Customers who formerly used the CSOs successfully transitioned to alternate service methods (PG&E GRC, A.22-04-016, 497018655, p. 1). By the end of 2021, 98% of customers had migrated to other payment channels, including mail (29%), internet (25%), pay by phone (24%), and neighborhood payment centers (17%) (PG&E GRC, A.22-04-016, 500175150, p. 6).\n",
      "*   **Proactive Outreach Model**: The CSO workforce transitioned to a proactive outreach model to assist vulnerable customers (PG&E GRC, A.22-04-016, 497018655, p. 1). This included outbound calls, \"case management\" support, and community-based organization (CBO) engagement (PG&E GRC, A.22-04-016, 475508067, p. 2).\n",
      "*   **Staffing Levels**: In March 2020, PG&E operated 75 CSOs with 212 positions (PG&E GRC, A.22-04-016, 497077047, p. 8). By May 2022, the CSO workforce had declined to 82 employees (PG&E GRC, A.22-04-016, 527305337, p. 26). PG&E believed that these staffing levels were appropriate, considering the accomplishments made with the COVID-19 outbound call campaign and the start of residential disconnections after the California Arrearage Payment Program (CAPP) ended (PG&E GRC, A.22-04-016, 527305337, p. 26).\n",
      "\n",
      "Would you like me to explore any aspect of this response in greater depth or address related regulatory considerations?\n",
      "\n",
      "Processing time: 4.45 seconds\n",
      "Prompt 16 complete.\n",
      "\n",
      "--- Running Prompt 17/60 ---\n",
      "\n",
      "GRC Assistant: I cannot answer this question. The available tools do not have the ability to provide information about specific customer behavior or payment channels.\n",
      "\n",
      "Processing time: 0.48 seconds\n",
      "Prompt 17 complete.\n",
      "\n",
      "--- Running Prompt 18/60 ---\n",
      "\n",
      "GRC Assistant: I cannot answer this question with the tools I have.\n",
      "\n",
      "Processing time: 0.41 seconds\n",
      "Prompt 18 complete.\n",
      "\n",
      "--- Running Prompt 19/60 ---\n",
      "\n",
      "GRC Assistant: I can provide information on PG&E's communication plans related to the CSO Closure and Transformation Proposal. However, I need more specific details to accurately answer your question about the communication methods PG&E will use within 90 days of the Commission's approval.\n",
      "\n",
      "To provide a comprehensive answer, I need to know:\n",
      "\n",
      "*   **Specific requirements**: Are there specific requirements outlined by the Commission regarding the communication methods PG&E must use?\n",
      "*   **Target audience**: Which customer segments are you interested in (e.g., vulnerable customers, all customers)?\n",
      "*   **Filing details:** Do you want details of the actual filing where PG&E describes these plans?\n",
      "\n",
      "Once I have this information, I can provide a detailed summary of the planned communication methods, supported by specific citations from the relevant documents.\n",
      "\n",
      "Processing time: 2.35 seconds\n",
      "Prompt 19 complete.\n",
      "\n",
      "--- Running Prompt 20/60 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "7bec644b2713b8ff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T22:12:50.665076Z",
     "start_time": "2025-05-07T22:12:50.638005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "from langgraph.graph import MessagesState, StateGraph\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import END\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langchain_core.documents import Document\n",
    "from langchain.chat_models import init_chat_model\n",
    "import time\n",
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "from datetime import datetime\n",
    "from testing.response_eval_tools import set_api_key_from_path, initialize_evaluation_llm, evaluate_model_outputs_from_paths\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import List, Dict, Any, Optional, Union\n",
    "import uuid\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from qa_pairs.qa_data_loader import load_static_demo_pairs\n",
    "set_api_key_from_path(Path('./testing/gemini-key'))\n",
    "llm = initialize_evaluation_llm()\n",
    "llm_session = LLMChatSession()"
   ],
   "id": "88ddfcf18006f93b",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T22:12:51.289765Z",
     "start_time": "2025-05-07T22:12:51.264455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def parse_prompt_blocks(file_path: str) -> List[Dict[str, Any]]:\n",
    "    path = Path(file_path)\n",
    "\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        file_content = f.read()\n",
    "\n",
    "    delimiter_pattern = r'=================\\s*Prompt\\s+(\\d+)\\s*=================\\n'\n",
    "\n",
    "    # Split the file content by prompt secitons\n",
    "    blocks = re.split(delimiter_pattern, file_content)\n",
    "\n",
    "    # First element will be empty or any text before the first delimiter\n",
    "    blocks = blocks[1:] if blocks[0].strip() == '' else blocks\n",
    "\n",
    "    # Process the blocks (pairs of [prompt_number, content])\n",
    "    results = []\n",
    "\n",
    "    # Handle paired elements (every odd index is a prompt number, every even index is content)\n",
    "    for i in range(0, len(blocks)-1, 2):\n",
    "        prompt_num = int(blocks[i])\n",
    "        content = blocks[i+1]\n",
    "\n",
    "        # Extract the prompt text\n",
    "        prompt_match = re.search(r'PROMPT:\\n(.*?)\\n\\n', content, re.DOTALL)\n",
    "        prompt_text = prompt_match.group(1) if prompt_match else \"Prompt text not found\"\n",
    "\n",
    "        # Get the response content\n",
    "        response = content[prompt_match.end():] if prompt_match else content\n",
    "\n",
    "        results.append({\n",
    "            \"prompt_num\": prompt_num,\n",
    "            \"prompt_text\": prompt_text,\n",
    "            \"response\": response,\n",
    "            \"full_block\": f\"================= Prompt {prompt_num} =================\\n{content}\"\n",
    "        })\n",
    "\n",
    "    return results"
   ],
   "id": "c2c77af1f3e3a1a0",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T22:37:25.253953Z",
     "start_time": "2025-05-07T22:37:25.211635Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import time\n",
    "import random\n",
    "import traceback\n",
    "from typing import List, Dict, Any, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "def build_evaluation_prompt(model_output_text: str) -> str:\n",
    "    prompt = f\"\"\"\n",
    "    You are evaluating a retrieval-augmented generation (RAG) system. For each Q/A pair in the data below, you will:\n",
    "    1. Assign a relevance score (0–5) based on how well the retrieved context matches the question.\n",
    "       - 0: No relevance, the context is completely off.\n",
    "       - 1: Very low relevance, context is weakly related.\n",
    "       - 2: Some relevance, context contains some useful details, but is not fully aligned.\n",
    "       - 3: Good relevance, context provides useful information that mostly answers the question.\n",
    "       - 4: Very good relevance, context almost entirely answers the question, with minimal extra information.\n",
    "       - 5: Perfect relevance, context directly and completely answers the question.\n",
    "\n",
    "    2. Assign a usage score (0–5) based on whether the context was properly used in generating the answer:\n",
    "       - 0: The context was not used at all.\n",
    "       - 1: The context was partially used but not enough to affect the response.\n",
    "       - 2: The context was mentioned but not effectively incorporated.\n",
    "       - 3: The context was used, but there were gaps in how it was integrated.\n",
    "       - 4: The context was effectively used, though some information might be missing.\n",
    "       - 5: The context was fully integrated and used to generate the response.\n",
    "\n",
    "    3. Optional: Identify and flag any hallucinations or unsupported claims. If the AI refers to information not found in the context, it should be noted.\n",
    "\n",
    "    Please format the output as a JSON array with the following fields: \"question\", \"relevance_score\", \"usage_score\", \"hallucination_notes\".\n",
    "\n",
    "    Here is the data to evaluate:\n",
    "    {model_output_text}\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def build_accuracy_evaluation_prompt(question: str, original_answer: str, model_answer: str) -> str:\n",
    "    prompt = f\"\"\"\n",
    "    You are evaluating the accuracy of an AI assistant's answer compared to a reference answer.\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Reference Answer: {original_answer}\n",
    "\n",
    "    AI Assistant's Answer: {model_answer}\n",
    "\n",
    "    Please evaluate the accuracy of the AI Assistant's answer on a scale from 0 to 5:\n",
    "\n",
    "    - 0: Completely incorrect or contradicts the reference answer.\n",
    "    - 1: Mostly incorrect with minimal accurate elements.\n",
    "    - 2: Partially correct but contains significant inaccuracies or missing critical information.\n",
    "    - 3: Moderately accurate with some minor inaccuracies or omissions.\n",
    "    - 4: Highly accurate with very minor omissions or imprecisions.\n",
    "    - 5: Perfectly accurate, capturing all key information from the reference answer.\n",
    "\n",
    "    Provide your assessment as a JSON object with the following fields:\n",
    "    - \"accuracy_score\": The numeric score (0-5)\n",
    "    - \"reasoning\": Brief explanation of your scoring decision\n",
    "    - \"key_differences\": Notable differences between the reference and assistant's answer\n",
    "\n",
    "    Evaluate based on semantic correctness, not exact wording. The AI Assistant's answer can be phrased differently and still be accurate if it conveys the same meaning.\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def evaluate_with_retry(prompt: str, llm) -> Optional[str]:\n",
    "    success = False\n",
    "\n",
    "    while not success:\n",
    "        try:\n",
    "            response = llm.invoke(prompt)\n",
    "            success = True\n",
    "            return response.content\n",
    "\n",
    "        except google_exceptions.ResourceExhausted as e:\n",
    "            error_string = str(e)\n",
    "\n",
    "            retry_match = re.search(r'retry_delay\\s*{\\s*seconds:\\s*(\\d+)', error_string)\n",
    "            if retry_match:\n",
    "                retry_seconds = int(retry_match.group(1))\n",
    "\n",
    "            print(f\"Waiting for {retry_seconds:.1f} seconds before retrying...\")\n",
    "\n",
    "            time.sleep(retry_seconds)\n",
    "\n",
    "def parse_json_response(response_text: str) -> List[Dict[str, Any]]:\n",
    "    if not response_text:\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        cleaned = str(response_text).strip()\n",
    "\n",
    "        # Remove json code blocks\n",
    "        cleaned = cleaned.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "\n",
    "        json_match = re.search(r'\\[\\s*{.+}\\s*\\]', cleaned, re.DOTALL)\n",
    "        if json_match:\n",
    "            cleaned = json_match.group(0)\n",
    "\n",
    "        # Load json\n",
    "        metrics = json.loads(cleaned)\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Failed to parse response as JSON: {e}\")\n",
    "        print(\"Raw content:\")\n",
    "        print(response_text)\n",
    "        return []\n",
    "\n",
    "\n",
    "def extract_model_answer(block_response: str) -> str:\n",
    "    # Try various patterns to extract the answer\n",
    "    answer_patterns = [\n",
    "        # Pattern: \"Answer: [answer text]\"\n",
    "        r'(?:answer|response):\\s*(.*?)(?:\\n\\n|\\Z)',\n",
    "        # Pattern: Q: [question] A: [answer]\n",
    "        r'Q:.*?A:\\s*(.*?)(?:\\n\\n|\\Z)',\n",
    "        # Pattern: \"The answer is [answer text]\"\n",
    "        r'(?:the answer is|the response is)\\s*(.*?)(?:\\n\\n|\\Z)',\n",
    "    ]\n",
    "\n",
    "    for pattern in answer_patterns:\n",
    "        match = re.search(pattern, block_response, re.IGNORECASE | re.DOTALL)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_answer_accuracy(question: str, original_answer: str, model_answer: str, llm) -> dict:\n",
    "    prompt = build_accuracy_evaluation_prompt(question, original_answer, model_answer)\n",
    "\n",
    "    response_content = evaluate_with_retry(prompt, llm)\n",
    "\n",
    "    if not response_content:\n",
    "        return {\n",
    "            \"accuracy_score\": 0,\n",
    "            \"reasoning\": \"Failed to evaluate due to technical issues\",\n",
    "            \"key_differences\": \"N/A\"\n",
    "        }\n",
    "\n",
    "    try:\n",
    "        # Clean up common JSON formatting issues\n",
    "        result = parse_json_response(response_content)\n",
    "\n",
    "        result[\"accuracy_score\"] = result.get(\"accuracy_score\", 0)\n",
    "        result[\"reasoning\"] = result.get(\"reasoning\", \"No reasoning provided\")\n",
    "        result[\"key_differences\"] = result.get(\"key_differences\", \"No differences noted\")\n",
    "\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to parse accuracy evaluation response: {e}\")\n",
    "        print(\"Raw content:\")\n",
    "        print(response_content)\n",
    "\n",
    "        # Return a default result\n",
    "        return {\n",
    "            \"accuracy_score\": 0,\n",
    "            \"reasoning\": \"Failed to parse evaluation\",\n",
    "            \"key_differences\": \"Error processing evaluation response\"\n",
    "        }\n",
    "\n",
    "\n",
    "def save_evaluation_results(results: List[Dict[str, Any]], output_path: str) -> None:\n",
    "    path = Path(output_path)\n",
    "\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Save the complete results\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"\\nEvaluation results saved to: {path.resolve()}\")\n",
    "\n",
    "    csv_path = path.with_suffix('.csv')\n",
    "\n",
    "    # Extract metrics into a flat structure\n",
    "    flat_metrics = []\n",
    "\n",
    "    for result in results:\n",
    "        block_num = result['block_info']['prompt_num']\n",
    "\n",
    "        for metric in result['evaluation_metrics']:\n",
    "            flat_metric = {\n",
    "                'prompt_num': block_num,\n",
    "                'question': metric.get('question', 'N/A'),\n",
    "                'relevance_score': metric.get('relevance_score', 'N/A'),\n",
    "                'usage_score': metric.get('usage_score', 'N/A'),\n",
    "                'answer_accuracy': metric.get('answer_accuracy', 'N/A'),\n",
    "                'accuracy_reasoning': metric.get('accuracy_reasoning', 'N/A')[:200] + '...' if metric.get('accuracy_reasoning') and len(metric.get('accuracy_reasoning')) > 200 else metric.get('accuracy_reasoning', 'N/A'),\n",
    "                'difficulty': metric.get('difficulty', 'N/A'),\n",
    "                'document_id': metric.get('document_id', 'N/A'),\n",
    "                'hallucination_notes': metric.get('hallucination_notes', '')\n",
    "            }\n",
    "            flat_metrics.append(flat_metric)\n",
    "\n",
    "    df = pd.DataFrame(flat_metrics)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Summary CSV saved to: {csv_path.resolve()}\")\n",
    "\n",
    "\n",
    "def evaluate_parsed_blocks(parsed_blocks: List[Dict[str, Any]], llm, original_qa_data: Optional[List[Dict[str, Any]]] = None, output_path: Optional[str] = None) -> List[Dict[str, Any]]:\n",
    "    all_metrics = []\n",
    "    results_with_block_info = []\n",
    "\n",
    "    # Map original Q&A data by question for quick lookup if provided\n",
    "    qa_lookup = {}\n",
    "    if original_qa_data:\n",
    "        # Create a lookup by normalizing questions (lowercase, remove punctuation)\n",
    "        for idx, qa_item in enumerate(original_qa_data):\n",
    "            # Create normalized question for matching\n",
    "            norm_q = re.sub(r'[^\\w\\s]', '', qa_item['question'].lower())\n",
    "            norm_q = re.sub(r'\\s+', ' ', norm_q).strip()\n",
    "            qa_lookup[norm_q] = (idx, qa_item)\n",
    "\n",
    "    for i, block in enumerate(parsed_blocks):\n",
    "        print(f\"\\nEvaluating block {i+1}/{len(parsed_blocks)} (Prompt #{block['prompt_num']})...\")\n",
    "\n",
    "        # Build evaluation prompt using the full_block content\n",
    "        prompt = build_evaluation_prompt(block['full_block'])\n",
    "\n",
    "        # Evaluate with retry\n",
    "        response_content = evaluate_with_retry(prompt, llm)\n",
    "\n",
    "        if response_content:\n",
    "            metrics = parse_json_response(response_content)\n",
    "\n",
    "            if metrics:\n",
    "                for metric in metrics:\n",
    "                    metric['prompt_num'] = block['prompt_num']\n",
    "\n",
    "                    # If we have original Q&A data, try to match and calculate accuracy\n",
    "                    if qa_lookup:\n",
    "                        norm_q = re.sub(r'[^\\w\\s]', '', metric['question'].lower())\n",
    "                        norm_q = re.sub(r'\\s+', ' ', norm_q).strip()\n",
    "\n",
    "\n",
    "                        idx, qa_item = qa_lookup[norm_q]\n",
    "                        matched_qa = (idx, qa_item)\n",
    "\n",
    "\n",
    "                        idx, qa_item = matched_qa\n",
    "                        metric['original_answer'] = qa_item['answer']\n",
    "                        metric['original_qa_index'] = idx\n",
    "                        metric['difficulty'] = qa_item.get('difficulty', 'unknown')\n",
    "                        metric['document_id'] = qa_item.get('document_identification', 'unknown')\n",
    "\n",
    "                        model_answer = extract_model_answer(block['response'])\n",
    "\n",
    "                        # LLM accuracy evaluation\n",
    "                        print(f\"  Evaluating answer accuracy for question: '{metric['question'][:50]}...'\")\n",
    "                        accuracy_result = evaluate_answer_accuracy(\n",
    "                            metric['question'],\n",
    "                            qa_item['answer'],\n",
    "                            model_answer,\n",
    "                            llm\n",
    "                        )\n",
    "\n",
    "                        metric['answer_accuracy'] = accuracy_result['accuracy_score']\n",
    "                        metric['accuracy_reasoning'] = accuracy_result['reasoning']\n",
    "                        metric['accuracy_key_differences'] = accuracy_result['key_differences']\n",
    "\n",
    "                all_metrics.extend(metrics)\n",
    "\n",
    "                results_with_block_info.append({\n",
    "                    'block_info': block,\n",
    "                    'evaluation_metrics': metrics,\n",
    "                    'raw_response': response_content\n",
    "                })\n",
    "\n",
    "                print(f\"Successfully evaluated block {i+1}. Found {len(metrics)} metric entries.\")\n",
    "            else:\n",
    "                print(f\"Failed to parse metrics for block {i+1}.\")\n",
    "        else:\n",
    "            print(f\"Failed to evaluate block {i+1} after multiple retries.\")\n",
    "\n",
    "    if output_path and results_with_block_info:\n",
    "        save_evaluation_results(results_with_block_info, output_path)\n",
    "\n",
    "    return all_metrics"
   ],
   "id": "8d8f85582aaef29a",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T22:37:46.906524Z",
     "start_time": "2025-05-07T22:37:26.230622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluate the parsed blocks\n",
    "input_file = \"./llm_output/test/llm_responses_with_context_qa_gemini_20250506_190637.txt\"\n",
    "output_file = \"./llm_output/evaluations/gemini_evaluation_20250506_190637__3.json\"\n",
    "parsed_blocks = parse_prompt_blocks(input_file)[0:10]\n",
    "print(f\"Found {len(parsed_blocks)} prompt blocks to evaluate.\")\n",
    "\n",
    "original_qa_data = test_data['qa_pairs'][0:10]\n",
    "\n",
    "evaluation_metrics = evaluate_parsed_blocks(parsed_blocks, llm, original_qa_data, output_file)\n",
    "\n",
    "# Calculate overall statistics\n",
    "if evaluation_metrics:\n",
    "    relevance_scores = [m.get('relevance_score', 0) for m in evaluation_metrics if isinstance(m.get('relevance_score'), (int, float))]\n",
    "    usage_scores = [m.get('usage_score', 0) for m in evaluation_metrics if isinstance(m.get('usage_score'), (int, float))]\n",
    "    accuracy_scores = [m.get('answer_accuracy', 0) for m in evaluation_metrics if isinstance(m.get('answer_accuracy'), (int, float))]\n",
    "\n",
    "    if relevance_scores:\n",
    "        avg_relevance = sum(relevance_scores) / len(relevance_scores)\n",
    "        print(f\"\\nAverage relevance score: {avg_relevance:.2f}/5\")\n",
    "\n",
    "    if usage_scores:\n",
    "        avg_usage = sum(usage_scores) / len(usage_scores)\n",
    "        print(f\"Average usage score: {avg_usage:.2f}/5\")\n",
    "\n",
    "    if accuracy_scores:\n",
    "        avg_accuracy = sum(accuracy_scores) / len(accuracy_scores)\n",
    "        print(f\"Average answer accuracy: {avg_accuracy:.2f}/5\")\n",
    "\n",
    "    hallucinations = sum(1 for m in evaluation_metrics if m.get('hallucination_notes') and m.get('hallucination_notes').strip())\n",
    "    print(f\"Number of entries with hallucinations: {hallucinations}/{len(evaluation_metrics)}\")\n",
    "\n",
    "    # Group metrics by difficulty if available\n",
    "    difficulty_groups = {}\n",
    "    for metric in evaluation_metrics:\n",
    "        difficulty = metric.get('difficulty', 'unknown')\n",
    "        if difficulty not in difficulty_groups:\n",
    "            difficulty_groups[difficulty] = []\n",
    "        difficulty_groups[difficulty].append(metric)\n",
    "\n",
    "    # Print stats by difficulty\n",
    "    print(\"\\nMetrics by difficulty:\")\n",
    "    for difficulty, metrics in difficulty_groups.items():\n",
    "        if metrics:\n",
    "            avg_rel = sum(m.get('relevance_score', 0) for m in metrics if isinstance(m.get('relevance_score'), (int, float))) / len(metrics)\n",
    "            avg_use = sum(m.get('usage_score', 0) for m in metrics if isinstance(m.get('usage_score'), (int, float))) / len(metrics)\n",
    "            avg_acc = sum(m.get('answer_accuracy', 0) for m in metrics if isinstance(m.get('answer_accuracy'), (int, float))) / len(metrics)\n",
    "            print(f\"  {difficulty.capitalize()} ({len(metrics)} questions):\")\n",
    "            print(f\"    Relevance: {avg_rel:.2f}/5\")\n",
    "            print(f\"    Usage: {avg_use:.2f}/5\")\n",
    "            print(f\"    Accuracy: {avg_acc:.2f}/5\")"
   ],
   "id": "e4575366bf263ea6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 prompt blocks to evaluate.\n",
      "\n",
      "Evaluating block 1/10 (Prompt #1)...\n",
      "  Evaluating answer accuracy for question: 'Who is the witness for the CSO Closure and Transfo...'\n",
      "Successfully evaluated block 1. Found 1 metric entries.\n",
      "\n",
      "Evaluating block 2/10 (Prompt #2)...\n",
      "  Evaluating answer accuracy for question: 'When did PG&E file its application to close its 65...'\n",
      "Successfully evaluated block 2. Found 1 metric entries.\n",
      "\n",
      "Evaluating block 3/10 (Prompt #3)...\n",
      "  Evaluating answer accuracy for question: 'Why did PG&E propose transitioning its CSO workfor...'\n",
      "Successfully evaluated block 3. Found 1 metric entries.\n",
      "\n",
      "Evaluating block 4/10 (Prompt #4)...\n",
      "  Evaluating answer accuracy for question: 'How did PG&E reassign its CSO workforce in respons...'\n",
      "Successfully evaluated block 4. Found 1 metric entries.\n",
      "\n",
      "Evaluating block 5/10 (Prompt #5)...\n",
      "  Evaluating answer accuracy for question: 'Yes/No: By the end of 2021, did 98% of exclusive C...'\n",
      "Successfully evaluated block 5. Found 1 metric entries.\n",
      "\n",
      "Evaluating block 6/10 (Prompt #6)...\n",
      "  Evaluating answer accuracy for question: 'Yes/No: Will PG&E submit its last CSO Annual Repor...'\n",
      "Successfully evaluated block 6. Found 1 metric entries.\n",
      "\n",
      "Evaluating block 7/10 (Prompt #7)...\n",
      "  Evaluating answer accuracy for question: 'What communication methods will PG&E use to inform...'\n",
      "Successfully evaluated block 7. Found 1 metric entries.\n",
      "\n",
      "Evaluating block 8/10 (Prompt #8)...\n",
      "  Evaluating answer accuracy for question: 'Why did PG&E include partnerships with community-b...'\n",
      "Successfully evaluated block 8. Found 1 metric entries.\n",
      "\n",
      "Evaluating block 9/10 (Prompt #9)...\n",
      "  Evaluating answer accuracy for question: 'Yes/No: Will PG&E post signage at the closed CSO l...'\n",
      "Successfully evaluated block 9. Found 1 metric entries.\n",
      "\n",
      "Evaluating block 10/10 (Prompt #10)...\n",
      "  Evaluating answer accuracy for question: 'How will PG&E measure and report the performance o...'\n",
      "Successfully evaluated block 10. Found 1 metric entries.\n",
      "\n",
      "Evaluation results saved to: C:\\Users\\Elijah\\PycharmProjects\\GRCResponder_tools\\llm_output\\evaluations\\gemini_evaluation_20250506_190637__3.json\n",
      "Summary CSV saved to: C:\\Users\\Elijah\\PycharmProjects\\GRCResponder_tools\\llm_output\\evaluations\\gemini_evaluation_20250506_190637__3.csv\n",
      "\n",
      "Average relevance score: 2.70/5\n",
      "Average usage score: 2.60/5\n",
      "Average answer accuracy: 1.50/5\n",
      "Number of entries with hallucinations: 5/10\n",
      "\n",
      "Metrics by difficulty:\n",
      "  Easy (5 questions):\n",
      "    Relevance: 2.40/5\n",
      "    Usage: 2.20/5\n",
      "    Accuracy: 1.20/5\n",
      "  Medium (4 questions):\n",
      "    Relevance: 3.75/5\n",
      "    Usage: 3.75/5\n",
      "    Accuracy: 2.25/5\n",
      "  Hard (1 questions):\n",
      "    Relevance: 0.00/5\n",
      "    Usage: 0.00/5\n",
      "    Accuracy: 0.00/5\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "39ada81316fc2cd4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
